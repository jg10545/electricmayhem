import numpy as np
import torch
import kornia.geometry.transform
import logging
import matplotlib.pyplot as plt
import matplotlib.patches

from ._implant import RectanglePatchImplanter
from electricmayhem._convenience import load_to_tensor

def get_warpmask_and_tfm(target_shape, patch_shape, coords, mask=1.):
    """
    Precompute information we'll need for warping and implanting patches- the transformation tensor
    for perspective transforms, and a binary mask showing which pixels are inside the distorted patch 
    corners.

    We use a separate masking step instead of just picking a weird color (like (0,255,0)) to use as a
    chromakey, otherwise aliasing effects will cause the chromakey color to bleed into the patch
    edges.

    Get a black-and-white mask showing which pixels are inside the distorted quadrangle corners
    :target_shape: length-3 tuple of target image shape; (C,H,W)
    :patch_shape: length-3 tuple of the patch shape; (C, H', W')
    :coords: [4,2] array or nested list of corner coordinates showing where the patch should go
        within the target image
    :mask: float or torch.Tensor representing the mask or alpha channel for the patch. Tensor should
        be (H',W'), (1, H', W'), or (C, H', W')

    Returns
    :warpmask: (1, C,H,W) tensor giving a mask of which pixels are inside
    :tfm: (1,3,3) perspective transform tensor
    """
    #patch_batch = torch.ones(shape).unsqueeze(0)
    if isinstance(mask, torch.Tensor):
        if len(mask.shape) == 2:
            mask = mask.unsqueeze(0) # (1, C', H')
    # patch_batch is a tensor with the shape of a patch. it's black if there's no mask
    # and lighter anywhere a mask or alpha channel should let the background show through
    patch_batch = 1-(torch.ones(patch_shape)*mask).unsqueeze(0) #(1, C, H', W')
    # compute patch border for the transformation
    patch_border = torch.tensor([[0.,0.],
                                 [patch_shape[2], 0], 
                                 [patch_shape[2], patch_shape[1]], 
                                 [0., patch_shape[1]]]).unsqueeze(0) # (1,4,2)
    tfm = kornia.geometry.transform.get_perspective_transform(patch_border, coords) # (1,3,3)
    # warp the black patch cutout onto a white background
    warpmask = kornia.geometry.transform.warp_perspective(patch_batch, tfm,
                                                      (target_shape[1], target_shape[2]),
                                                      padding_mode="fill", 
                                                      fill_value=torch.tensor([1,1,1])) # (1,C,H,W)
    return warpmask, tfm


def unpack_warp_dataframe(df, patch_shapes, mask=1.):
    """
    Precompute and organize transformation matrices and warp masks for an entire dataset

    :df: pandas dataframe containing box coordinates
    :patch_shapes: 3-tuple giving patch shape, or dict of 3-tuples
    :mask: float, tensor, or dictionary mapping patch names to floats/tensors 
    """
    im_filename_list = list(df.image.unique())

    if not isinstance(patch_shapes, dict):
        patch_shapes = {k:patch_shapes for k in df.patch.unique()}
    
    tfms = {}
    warpmasks = {}
    patchnames = {}
    target_images = {i:load_to_tensor(i) for i in im_filename_list}
    target_shape = target_images[im_filename_list[0]].shape
    # for each target image
    for i in im_filename_list:
        tfms[i] = []
        warpmasks[i] = []
        patchnames[i] = []
        # precompute warp mask and perspective warp tensor for 
        # each annotation for that target image
        for e,r in df[df.image == i].iterrows():
            coords = torch.tensor([[[r.uly, r.ulx], [r.lly, r.llx], [r.lry, r.lrx], [r.ury, r.urx]]]).type(torch.float32)
            if isinstance(mask, dict):
                m = mask[r.patch]
            else:
                m = mask
            warpmask, tfm = get_warpmask_and_tfm(target_shape, patch_shapes[r.patch], coords, mask=m)
            warpmasks[i].append(warpmask)
            tfms[i].append(tfm)
            patchnames[i].append(r.patch)
        warpmasks[i] = torch.concatenate(warpmasks[i], 0)
        tfms[i] = torch.stack(tfms[i], 0)

    return im_filename_list, tfms, warpmasks, patchnames, target_images

def warp_and_implant_single(patch, target, tfm, warpmask, scale_brightness=False):
    """
    Implant a single patch into a single target image. This may wind up being slower and less memory efficient
    than the batched approach, but gives the flexibility to have target images with varying combinations of 
    patches in them
    :patch: (C,H',W') tensor containing a patch
    :target: (C,H,W) tensor containing target image
    :tfm: (1,3,3) tensor containing transformation; generated by kornia.geometry.transform.get_perspective_transform()
    :warpmask: (H,W) tensor containing a mask of where the patch should go within the image
    :scale_brightness: if True, adjust brightness of patch to match the average brightness of the section of
            image it's replacing
    """
    patch = patch.unsqueeze(0) # (B, C, H', W')
    target = target.unsqueeze(0)
    # apply transformation to get a warped patch with green background
    warped_patch = kornia.geometry.transform.warp_perspective(patch, tfm,
                                                      (target.shape[2], target.shape[3]),
                                                      padding_mode="border") # (B,C,H,W)    
    # do we need to scale the patch's brightness?
    if scale_brightness:
        with torch.no_grad():
            # compute the brightness of each patch in the batch
            patch_brightness = torch.sum(warped_patch*(1-warpmask), dim=(1,2,3), 
                                      keepdim=True)/torch.sum(1-warpmask, dim=(1,2,3),
                                                              keepdim=True) # (B,1,1,1)
            # target_batch*(1-warpmask) will be the target images in all the places where we're
            # overwriting with patch. we have to take the sum and divide by the sum of 1-warpmask
            # to get an average.
            target_brightness = torch.sum(target*(1-warpmask), dim=(1,2,3), 
                                      keepdim=True)/torch.sum(1-warpmask, dim=(1,2,3),
                                                              keepdim=True) # (B,1,1,1)
            scale = target_brightness/patch_brightness # (B,1,1,1)
    else:
        scale = 1.
    return (target*warpmask + warped_patch*(1-warpmask)*scale).squeeze(0)

def warp_and_implant_batch(patch_batch, target_batch, coord_batch, mask=None,
                           scale_brightness=False):
    """
    This version is deprecated- leaving the code in to steal from in case we want to add a more 
    memory-efficient version that doesn't need to handle uneven combinations of patches in each image

    :patch_batch: (B,C,H',W') tensor containing batch of patches
    :target_batch: (B,C,H,W) tensor containing batch of target images
    :coord_batch: (B,4,2) tensor containing corner coordinates for implanting the patch in each image
    :mask: optional, batch of masks
    :scale_brightness: if True, adjust brightness of patch to match the average brightness of the section of
            image it's replacing
    """
    assert patch_batch.shape[0] == target_batch.shape[0], "batch dimensions need to line up"
    assert target_batch.shape[0] == coord_batch.shape[0], "batch dimensions need to line up"
    
    # get transformation matrix
    patch_border = torch.tensor([[0.,0.],
                                 [patch_batch.shape[3], 0], 
                                 [patch_batch.shape[3], patch_batch.shape[2]], 
                                 [0., patch_batch.shape[2]]]) # (4,2)
    patch_border = torch.stack([patch_border for _ in range(patch_batch.shape[0])],0).to(patch_batch.device) # (B,4,2)
    
    tfm = kornia.geometry.transform.get_perspective_transform(patch_border, coord_batch) # (B,3,3)
    # apply transformation to get a warped patch with green background
    warped_patch = kornia.geometry.transform.warp_perspective(patch_batch, tfm,
                                                      (target_batch.shape[2], target_batch.shape[3]),
                                                      padding_mode="border") # (B,C,H,W)
    # do the transformation a second time- but with a "green screen" we can use to generate the mask
    # we'll need to implant in the image (not to be confused with an additional optional mask for 
    # within the boundaries of the patch itself). the reason we do this with a separate step is otherwise
    # the image resampling causes some of the chromakey to leak into the implanted patch.
    with torch.no_grad():
        chromakey = kornia.geometry.transform.warp_perspective(patch_batch, tfm,
                                                      (target_batch.shape[2], target_batch.shape[3]),
                                                      padding_mode="fill", 
                                                      fill_value=torch.tensor([0,1,0])) # (B,C,H,W)
        # use the green background to create a mask for deciding where to overwrite the target image
        # with the patch
        warpmask = ((chromakey[:,0,:,:] == 0)&(chromakey[:,1,:,:] == 1)&(chromakey[:,2,:,:] == 0)).type(torch.float32) # (B,H,W)
        warpmask = warpmask.unsqueeze(1) # (B,1,H,W)
        # so every place where warpmask=1 will be the target image; every place where it's 0 will be the patch
    
    # do we need to scale the patch's brightness?
    if scale_brightness:
        with torch.no_grad():
            # compute the brightness of each patch in the batch
            patch_brightness = torch.sum(warped_patch*(1-warpmask), dim=(1,2,3), 
                                      keepdim=True)/torch.sum(1-warpmask, dim=(1,2,3),
                                                              keepdim=True) # (B,1,1,1)
            # target_batch*(1-warpmask) will be the target images in all the places where we're
            # overwriting with patch. we have to take the sum and divide by the sum of 1-warpmask
            # to get an average.
            target_brightness = torch.sum(target_batch*(1-warpmask), dim=(1,2,3), 
                                      keepdim=True)/torch.sum(1-warpmask, dim=(1,2,3),
                                                              keepdim=True) # (B,1,1,1)
            scale = target_brightness/patch_brightness # (B,1,1,1)
    else:
        scale = 1.

    if mask is not None:
        with torch.no_grad():
            # IMAGE MASK CASE
            if isinstance(mask, torch.Tensor):
                # add batch dimension if necessary
                if len(mask.shape) == 3:
                    mask = torch.stack([mask for _ in range(patch_batch.shape[0])], 0)
                # apply same transforms to batch of masks, but fill with zeros. patch will only show through
                # in places where mask > 0
                mask_pw = kornia.geometry.transform.warp_perspective(mask, tfm,
                                                      (target_batch.shape[2], target_batch.shape[3]),
                                                       padding_mode="fill", 
                                                      fill_value=torch.tensor([1,1,1]))
                
                # update the warp mask to exclude the patch wherever the mask is zero
                warpmask = mask_pw#1 - ((1-warpmask)*mask_pw)
            # SCALAR MASK CASE
            else:
                # we want every place where warpmask=1 to still be 1.
                # we want every place where warpmask=0 to be the scalar mask value
                # mask=1 should leave warpmask unchanged and mask=0 should be 1 everywhere
                warpmask = warpmask*mask + 1 - mask

    
    return torch.clamp(target_batch*warpmask + warped_patch*(1-warpmask)*scale, 0, 1) # brightness scaling could push above 1


def scale_coordinate_list(coord, scale=0.1):
    """
    Utility function to scale a list of corner coordinates out by a fraction
    of its width
    """
    newcoord = [[0,0], [0,0], [0,0], [0,0]] # upper left, upper right, lower right, lower left

    newcoord[0][0] = int(coord[0][0] - scale*(coord[1][0]-coord[0][0])) # upper left x
    newcoord[0][1] = int(coord[0][1] - scale*(coord[3][1]-coord[0][1])) # upper left y

    newcoord[1][0] = int(coord[1][0] - scale*(coord[0][0]-coord[1][0])) # upper right x
    newcoord[1][1] = int(coord[1][1] - scale*(coord[3][1]-coord[0][1])) # upper right y

    newcoord[3][0] = int(coord[3][0] - scale*(coord[1][0]-coord[0][0])) # lower left x
    newcoord[3][1] = int(coord[3][1] - scale*(coord[0][1]-coord[3][1])) # lower left y

    newcoord[2][0] = int(coord[2][0] - scale*(coord[0][0]-coord[1][0])) # upper right x
    newcoord[2][1] = int(coord[2][1] - scale*(coord[0][1]-coord[3][1])) # upper right y
    
    return newcoord


class WarpPatchImplanter(RectanglePatchImplanter):
    """
    Class for adding a patches to target images, with arbitrary corners that may be warped from
    a rectangle. Assume all target images are the same dimensions.
    
    This implementation allows for arbitrary combinations of patches and targets- a particular patch
    could be implanted multiple times in a given target image or not at all. This flexibility comes 
    at some overhead to memory and compute cost, so there may be value in a less-restrictive but 
    higher-performance version of this at some point.

    This class also precomputes warped versions of masks to save time and memory during training- but
    this means that the masks have to be fixed during training, and this class would need to be modified
    to dynamically learn patch shapes.

    The heavy lifting of this class is being done by the perspective warping tools in kornia.geometry. We'll
    precompute the transformations as well as a "warpmask" for each target image/patch combination. In earlier
    experiments I used a chromakey to do the masking in fewer steps, but it led to some noticeable aliasing
    effects (chromakey color leakage around borders of the patch) so this is the least-complicated version
    I've found without that problem.
    """
    name = "WarpPatchImplanter"
    
    def __init__(self, df, patch_shapes, mask=1, scale_brightness=False, dataset_name=None):
        """
        :df: dataframe containing an "image" column with paths to images, x and y coordinates for 
            each corner of the distorted box, and (optionally) "patch" column (specifying which patch name the
            box is for) and "split" column (train/eval). So the minimal columns expected are image, llx, lly,
            ulx, uly, lrx, lry, urx, and ury. Optional columns are patch and split.
        :patch_shapes: 3-tuple or dictionary of 3-tuples giving the shape of patches that will be passed. Used
            to pre-compute transformation matrices.
        :mask: scalar between 0 and 1, torch.Tensor on the unit interval, or dictionary of any of these values
             to use for masking the patch
        :scale_brightness: if True, adjust brightness of patch to match the average brightness of the section of
            image it's replacing
        :dataset_name: None or str; name of dataset to be logged in mlflow
        """
        super(RectanglePatchImplanter, self).__init__()
        df = df.copy()
        if "patch" not in df.columns:
            df["patch"] = "patch"
        if not isinstance(patch_shapes, dict):
            patch_shapes = {"patch":patch_shapes}
        self.patch_keys = list(patch_shapes.keys())
        self.df = df
        self._dataset_name = dataset_name
        
        with torch.no_grad():
            # precompute transformation matrices and warp masks
            if "split" not in df.columns:
                logging.warning("no 'split' column found in dataset; using same images for train and eval")
                self.im_filename_list, self.tfms, self.warpmasks, self.patch_names, self.target_images = unpack_warp_dataframe(df, patch_shapes, mask=mask)
                self.eval_im_filename = self.im_filename_list
                self.eval_tfms = self.tfms
                self.eval_warpmasks = self.warpmasks
                self.eval_target_images = self.target_images
                self.eval_patch_names = self.patch_names
            else:
                self.im_filename_list, self.tfms, self.warpmasks, self.patch_names, self.target_images = unpack_warp_dataframe(df[df.split == "train"], patch_shapes, mask=mask)
                self.eval_im_filename_list, self.eval_tfms, self.eval_warpmasks, self.eval_patch_names, self.eval_target_images = unpack_warp_dataframe(df[df.split != "train"], patch_shapes, mask=mask)
            
            self.mask = mask

        self.params = {"scale_brightness":scale_brightness}



    def get_min_dimensions(self):
        assert False, "not implemented for this implanter"
        
        
    def validate(self, patch):
        """
        Check to see whether any of your patch/scale/image/box combinations could throw an error

        NOT YET UPDATED FOR NEW API
        """
        assert False, "not yet implemented"
        all_validated = True
        
        for i in range(len(self.images)):
            for j in range(len(self.boxes[i])):
                b = self.boxes[i][j]
                box_ok = True
                # should be four corners in the box
                if len(b) != 4:
                    box_ok = False
                # each corner should have two coordinates
                for k in b:
                    if len(k) != 2:
                        box_ok = False
                if not box_ok:
                    logging.warning(f"{self.name}: box {j} of image {self.imgkeys[i]} has the wrong shape")
                    all_validated = False
        return all_validated
    

    
    def forward(self, patches, control=False, evaluate=False, params=None, **kwargs):
        """
        Implant a batch of patches in a batch of images
        
        :patches: torch Tensor containing a batch of patches, or a dictionary of patch batches
        :control: if True, leave the patches off (for diagnostics)
        :params: dictionary of params to override random sampling
        :kwargs: passed to self.sample()
        """
        if params is None:
            params = {}
        if evaluate:
            im_filename_list = self.eval_im_filename_list
            target_images = self.eval_target_images
            tfms = self.eval_tfms
            warpmasks = self.eval_warpmasks
            patch_names = self.eval_patch_names
        else:
            im_filename_list = self.im_filename_list
            target_images = self.target_images
            tfms = self.tfms
            warpmasks = self.warpmasks
            patch_names = self.patch_names

        if isinstance(patches, torch.Tensor):
            patches = {"patch":patches}

        # find which device the patches are on
        device = patches[self.patch_keys[0]].device

        # infer batch size from input patch
        N = patches[self.patch_keys[0]].shape[0]
        
        if control:
            params = self.lastsample
        else:
            # sample parameters if necessary
            if "image" not in params:
                params["image"] = np.random.choice(im_filename_list, size=N, replace=True)
            self.lastsample = params

        # copy all the target images in the batch to the right device
        with torch.no_grad():
            targets = torch.stack([target_images[params["image"][n]].clone().detach() 
                                   for n in range(N)], 0).to(device)

        # build a batch of target images
        batch = []
        # for each batch element
        for n in range(N):
            k = params["image"][n]
            # grab the target image
            target = targets[n]
            if not control:
                # for each patch we need to implant in the image. tfms should already be on the GPU
                # but we copy warpmasks each time
                for t, w, p in zip(tfms[k].clone().detach().to(device), 
                                   warpmasks[k].clone().detach().to(device), patch_names[k]):
                    target = warp_and_implant_single(patches[p][n], target, t, w,
                                                     scale_brightness=self.params["scale_brightness"])
            batch.append(target)

        batch = torch.stack(batch, 0)
        return torch.clamp(batch, 0, 1), kwargs
    
    def plot_boxes(self, evaluate=False):
        """
        Quick visualization with matplotlib of the victim images and box regions
        """
        assert False, "not yet implemented"
        if evaluate:
            images = self.eval_images
            boxes = self.eval_boxes
            imgkeys = self.eval_imgkeys
        else:
            images = self.images
            boxes = self.boxes
            imgkeys = self.imgkeys
            
        n = len(images)
        d = int(np.ceil(np.sqrt(n)))
        fig, axs = plt.subplots(nrows=d, ncols=d, squeeze=False)

        i = 0
        for axrow in axs:
            for ax in axrow:
                ax.set_axis_off()
                if i < n:
                    ax.imshow((images[i].permute(1,2,0).detach().cpu().numpy()))
                    
                    for j in range(len(boxes[i])):
                        b = boxes[i][j]
                        ax.plot([f[0] for f in b], [f[1] for f in b], "o-")
                    ax.set_title(imgkeys[i])
                
                i += 1
        return fig
    
    def get_last_sample_as_dict(self):
        """
        Return last sample as a JSON-serializable dict
        """
        return self.lastsample
        """
        if self._eval_last:
            imgkeys = self.eval_imgkeys
        else:
            imgkeys = self.imgkeys
        outdict = {}
        for k in self.lastsample:
            if k == "image":
                outdict["image"] = [imgkeys[i] for i in self.lastsample["image"].cpu().detach().numpy()]
            else:
                outdict[k] = [float(i) for i in self.lastsample[k].cpu().detach().numpy()]
        return outdict"""
    

class DEPRECATEDWarpPatchImplanter(RectanglePatchImplanter):
    """
    Class for adding a patch to an image, with arbitrary corners that may be warped from
    a rectangle. Assume all target images are the same dimensions.
    
    
    If eval_imagedict and eval_boxdict aren't passed, evaluation will be done
    one training images/boxes.
    """
    name = "WarpPatchImplanter"
    
    def __init__(self, imagedict, boxdict, eval_imagedict=None,
                 eval_boxdict=None, mask=None, scale_brightness=False):
        """
        :imagedict: dictionary mapping keys to images, as PIL.Image objects or 3D numpy arrays
        :boxdict: dictionary mapping the same keys to lists of corner coordinates, i.e.
            {"img1":[[[x1,y1],[x2,y2],[x3,y3],[x4,y4]], ...]}
            The order of (x,y) pairs is [upper left, upper right, lower right, lower left]
        :eval_imagedict: separate dictionary of images to evaluate on
        :eval_boxdict: separate dictionary of lists of bounding boxes for evaluation
        :scale: tuple of floats; range of scaling factors
        :offset_frac_x: None or float between 0 and 1- optionally specify a relative x position within the target box for the patch.
        :offset_frac_y: None or float between 0 and 1- optionally specify a relative y position within the target box for the patch.
        :mask: None, scalar between 0 and 1, or torch.Tensor on the unit interval to use for masking the patch
        :scale_brightness: if True, adjust brightness of patch to match the average brightness of the section of
            image it's replacing
        """
        super().__init__(imagedict, boxdict, eval_imagedict=eval_imagedict,
                         eval_boxdict=eval_boxdict, mask=mask,
                         scale_brightness=scale_brightness)
        
        self._sample_scale = False # whether self.sample() should sample a random scaling factor for patch
        self._sample_offsets = False # whether self.sample() should sample offsets relative to the box

        # some of the parameters in the parent class aren't used here- let's remove some clutter
        for key in ["scale", "offset_frac_x", "offset_frac_y"]:
            if key in self.params:
                del(self.params[key])



    def get_min_dimensions(self):
        assert False, "not implemented for this implanter"
        
    def DEPRECATED_sample(self, n, evaluate=False, **kwargs):
        """
        Sample implantation parameters for batch size n, overwriting with
        kwargs if necessary.
        """
        p = self.params
        
        if evaluate:
            images = self.eval_images
            boxes = self.eval_boxes
            self._eval_last = True
        else:
            images = self.images
            boxes = self.boxes
            self._eval_last = False
        
        sampdict = {k:kwargs[k] for k in kwargs}
        if "image" not in kwargs:
            sampdict["image"] = torch.randint(low=0, high=len(images), size=[n])
        if "box" not in kwargs:
            i = torch.tensor([torch.randint(low=0, high=len(boxes[j]), size=[]) for j in sampdict["image"]])
            sampdict["box"] = i

        self.lastsample = sampdict
        
    def validate(self, patch):
        """
        Check to see whether any of your patch/scale/image/box combinations could throw an error
        """
        all_validated = True
        
        for i in range(len(self.images)):
            for j in range(len(self.boxes[i])):
                b = self.boxes[i][j]
                box_ok = True
                # should be four corners in the box
                if len(b) != 4:
                    box_ok = False
                # each corner should have two coordinates
                for k in b:
                    if len(k) != 2:
                        box_ok = False
                if not box_ok:
                    logging.warning(f"{self.name}: box {j} of image {self.imgkeys[i]} has the wrong shape")
                    all_validated = False
        return all_validated
    
    def _get_mask(self, patch):
        """
        Input a patch and return a mask, either as a scalar
        or as a (1,H,W) tensor with the same spatial dimensions
        as the patch.

        This method will handle cases where either no mask was
        specified, a scalar mask was specified, or a mask image
        was specified as a 2D or 3D tensor
        """
        # no mask? easy
        if self.mask is None:
            return 1.
        else:
            # if mask is a tensor...
            if isinstance(self.mask, torch.Tensor)|isinstance(self.mask, torch.nn.Parameter):
                with torch.no_grad():
                    mask = self.mask.clone().detach().type(torch.float32)
                    # if mask was specified as 2D, add a batch dimension so
                    # it will broadcast directly
                    if len(mask.shape) == 2:
                        mask = mask.unsqueeze(0)
                    # resize mask to match patch
                    mask = kornia.geometry.resize(mask, (patch.shape[-2], patch.shape[-1]))
            # scalar mask case
            else:
                mask = self.mask
        return mask

    
    def forward(self, patches, control=False, evaluate=False, params={}, **kwargs):
        """
        Implant a batch of patches in a batch of images
        
        :patches: torch Tensor; stack of patches
        :control: if True, leave the patches off (for diagnostics)
        :params: dictionary of params to override random sampling
        :kwargs: passed to self.sample()
        """
        if evaluate:
            images = self.eval_images
            boxes = self.eval_boxes
        else:
            images = self.images
            boxes = self.boxes
        
        if control:
            params = self.lastsample
        # sample parameters if necessary
        self.sample(patches.shape[0], evaluate=evaluate, **params)

        # dictionary of sampled parameters
        s = self.lastsample

        # get mask
        mask = self._get_mask(patches)

        # build a batch of target images
        target_images = torch.stack([images[s["image"][i]] 
                                     for i in range(patches.shape[0])], 0).to(patches.device)
        
        # if it's a control batch, skip the implanting step
        if control:
            return target_images, kwargs

        # build a batch of box coordinates
        coords = torch.stack([torch.tensor(boxes[s["image"][i]][s["box"][i]]).type(torch.float32)
                              for i in range(patches.shape[0]) ],0).to(patches.device)

        # implant patch
        implanted_images = warp_and_implant_batch(patches, target_images, coords, mask=mask,
                                                  scale_brightness=self.params["scale_brightness"])

        return implanted_images, kwargs
    
    def plot_boxes(self, evaluate=False):
        """
        Quick visualization with matplotlib of the victim images and box regions
        """
        if evaluate:
            images = self.eval_images
            boxes = self.eval_boxes
            imgkeys = self.eval_imgkeys
        else:
            images = self.images
            boxes = self.boxes
            imgkeys = self.imgkeys
            
        n = len(images)
        d = int(np.ceil(np.sqrt(n)))
        fig, axs = plt.subplots(nrows=d, ncols=d, squeeze=False)

        i = 0
        for axrow in axs:
            for ax in axrow:
                ax.set_axis_off()
                if i < n:
                    ax.imshow((images[i].permute(1,2,0).detach().cpu().numpy()))
                    
                    for j in range(len(boxes[i])):
                        b = boxes[i][j]
                        ax.plot([f[0] for f in b], [f[1] for f in b], "o-")
                    ax.set_title(imgkeys[i])
                
                i += 1
        return fig
    
    def get_last_sample_as_dict(self):
        """
        Return last sample as a JSON-serializable dict
        """
        if self._eval_last:
            imgkeys = self.eval_imgkeys
        else:
            imgkeys = self.imgkeys
        outdict = {}
        for k in self.lastsample:
            if k == "image":
                outdict["image"] = [imgkeys[i] for i in self.lastsample["image"].cpu().detach().numpy()]
            else:
                outdict[k] = [float(i) for i in self.lastsample[k].cpu().detach().numpy()]
        return outdict
    