# whitebox physical patch pipeline## Usage### To train a patch* Initialize all the steps of your pipeline* Combine into a `Pipeline` object* Call `pipeline.set_logging()` to point to a log directory and MLFlow server* Define a loss function and call `pipeline.set_loss()` to add it to the pipeline* Call `pipeline.initialize_patch_params()` to create a new patch (or latent vector for a patch)* Call `pipeline.cuda()` to copy to GPU* Call `pipeline.train()` to train the patch### To optimize hyperparameters* Initialize all the steps of your pipeline* Combine into a `Pipeline` object* Define a loss function and call `pipeline.set_loss()` to add it to the pipeline* Call `pipeline.cuda()` to copy to GPU* Call `pipeline.optimize()` to start training patches with randomly-selected hyperparameters## General pipeline stages* **create:** create a patch from some parameterization of the patch* **implant:** incorporate the patch into a target image* **compose:** create the final image that will be passed through the target model* **infer:** run the composed image through one or more models* **loss:** compute loss and performance metrics on the model result; backpropagate gradients.## Steps implemented so far* `PatchResizer`: upsample a low-res patch* `PatchStacker`: stack a 1-channel patch into a 3-channel patch* `RectanglePatchImplanter`: implant a patch into a target image. Can randomly select from multiple images and multiple bounding boxes per image. Optionally, reserve some target images only for evaluation.* `KorniaAugmentationPipeline`: wraps the `kornia` library to augment an image* `ModelWrapper`: wrap a pytorch model as a pipeline step## ExtendingEvery pipeline step should subclass `electricmayhem._pipeline.PipelineBase`, which in turn subclasses `toch.nn.Module`. Make sure:* There's a `name` attribute that's a string.* There should be an `__init__()` method that calls `super().__init__()`. * Any keyword arguments you need to re-initialize the step should be captured in a JSON/YAML-serializable dict in `self.params`.* There should be a `forward()` method that does a few things:  * If called with `control=True`, runs a control batch (same configuration as previous batch but without the patch)  * If called with `evaluate=True`, runs an evaluation batch (for example possibly using holdout images or a separate model)  * If a dictionary of paramaters is passed to the `params` kwarg, overrules any randomly-sampled parameters with these values.* There should be a `get_last_sample_as_dict()` method. It should return any stochastic parameters sampled for the last batch as a dictionary containing lists or 1D `numpy` arrays of length `batchsize`* Optionally, overwrite the `get_description()` method to generate a more useful markdown description for MLFlow.```class MyPipelineStage(PipelineBase):    name = "MyPipelineStage"    def __init__(self, foo, bar):        super().__init__()        self.params = {"foo":foo, "bar":bar}            def forward(self, x, control=False, evaluate=False, params=None, **kwargs):        stuff here        y = f(x)        return y            def get_last_sample_as_dict(self):        return dict(some stuff)        ```