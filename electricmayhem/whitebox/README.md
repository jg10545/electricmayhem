# whitebox physical patch pipeline## Usage### To train a patch* Initialize all the steps of your pipeline* Combine into a `Pipeline` object* Call `pipeline.set_logging()` to point to a log directory and MLFlow server* Define a loss function and call `pipeline.set_loss()` to add it to the pipeline.   * The loss function should have two inputs: the output of the pipeline, and the patch parameters  * The loss function should output a dictionary mapping loss function terms to UNREDUCED values. So each value in the dictionary should be a tensor of length `batch_size`  * `pipeline.set_loss()` will run a random patch through to test the loss function; you may need to manually specify the shape.* Call `pipeline.initialize_patch_params()` to create a new patch (or latent vector for a patch)* Call `pipeline.cuda()` to copy to GPU* Call `pipeline.train()` to train the patch. Main hyperparameters you'll need to specify:  * batch size  * number of steps  * loss function weights- you'll need to pass at least one of the keys from the dictionary your loss function outputs (summary stats on all the loss dict outputs will still be stored in TensorBoard though)### To optimize hyperparameters* Initialize all the steps of your pipeline* Combine into a `Pipeline` object* Define a loss function and call `pipeline.set_loss()` to add it to the pipeline* Call `pipeline.cuda()` to copy to GPU* Call `pipeline.optimize()` to start training patches with randomly-selected hyperparameters. Specify hyperparameters the same way you would for `pipeline.train()`, except:  * replace the hyperparameter value with a tuple `(low,high)` to optimize that value (on a linear scale)  * replace with a tuple `(low, high, 'log')` to optimize on a log scale  * replace with a tuple `(low, high, 'int')` to optimize as an integer (for example, for the number of gradient accumulation steps)## General pipeline stages* **create:** create a patch from some parameterization of the patch* **implant:** incorporate the patch into a target image* **compose:** create the final image that will be passed through the target model* **infer:** run the composed image through one or more models* **loss:** compute loss and performance metrics on the model result; backpropagate gradients.## Steps implemented so far* `PatchResizer`: upsample a low-res patch* `PatchStacker`: stack a 1-channel patch into a 3-channel patch* `RectanglePatchImplanter`: implant a patch into a target image. Can randomly select from multiple images and multiple bounding boxes per image. Optionally, reserve some target images only for evaluation.* `KorniaAugmentationPipeline`: wraps the `kornia` library to augment an image* `ModelWrapper`: wrap a pytorch model as a pipeline step## ExtendingEvery pipeline step should subclass `electricmayhem._pipeline.PipelineBase`, which in turn subclasses `toch.nn.Module`. Make sure:* There's a `name` attribute that's a string.* There should be an `__init__()` method that calls `super().__init__()`. * Any keyword arguments you need to re-initialize the step should be captured in a JSON/YAML-serializable dict in `self.params`.* There should be a `forward()` method that does a few things:  * If called with `control=True`, runs a control batch (same configuration as previous batch but without the patch)  * If called with `evaluate=True`, runs an evaluation batch (for example possibly using holdout images or a separate model)  * If a dictionary of paramaters is passed to the `params` kwarg, overrules any randomly-sampled parameters with these values.* There should be a `get_last_sample_as_dict()` method. It should return any stochastic parameters sampled for the last batch as a dictionary containing lists or 1D `numpy` arrays of length `batchsize`* Optionally, overwrite the `get_description()` method to generate a more useful markdown description for MLFlow.* Optionally, overwrite the `log_vizualizations()` method with any diagnostics that would be useful to log to TensorBoard. This method will get called whenever `pipeline.evluate()` is run.```class MyPipelineStage(PipelineBase):    name = "MyPipelineStage"    def __init__(self, foo, bar):        super().__init__()        self.params = {"foo":foo, "bar":bar}            def forward(self, x, control=False, evaluate=False, params=None, **kwargs):        <stuff here>        y = f(x)        return y            def get_last_sample_as_dict(self):        return dict(<some stuff>)             def log_vizualizations(self, x, writer, step):         """         """         writer.add_image("stacked_patch", <some stuff>, global_step=step)        ```