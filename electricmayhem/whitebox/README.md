# whitebox physical patch pipeline## General pipeline stages* **create:** create a patch from some parameterization of the patch* **implant:** incorporate the patch into a target image* **compose:** create the final image that will be passed through the target model* **infer:** run the composed image through one or more models* **loss:** compute loss and performance metrics on the model result; backpropagate gradients.## ExtendingEvery pipeline step should subclass `electricmayhem._pipeline.PipelineBase`, which in turn subclasses `toch.nn.Module`. Make sure:* There's a `name` attribute that's a string.* There should be an `__init__()` method that calls `super().__init__()`. * Any keyword arguments you need to re-initialize the step should be captured in a JSON/YAML-serializable dict in `self.params`.* There should be a `forward()` method that does a few things:  * If called with `control=True`, runs a control batch (same configuration as previous batch but without the patch)  * If called with `evaluate=True`, runs an evaluation batch (for example possibly using holdout images or a separate model)  * If a dictionary of paramaters is passed to the `params` kwarg, overrules any randomly-sampled parameters with these values.* There should be a `get_last_sample_as_dict()` method. It should return any stochastic parameters sampled for the last batch as a dictionary containing lists or 1D `numpy` arrays of length `batchsize`* Optionally, overwrite the `get_description()` method to generate a more useful markdown description for MLFlow.```class MyPipelineStage(PipelineBase):    name = "MyPipelineStage"    def __init__(self, foo, bar):        super().__init__()        self.params = {"foo":foo, "bar":bar}            def forward(self, x, control=False, evaluate=False, params=None, **kwargs):        stuff here        y = f(x)        return y            def get_last_sample_as_dict(self):        return dict(some stuff)        ```