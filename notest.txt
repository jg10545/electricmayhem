Evaluation setup* This code expects a fixed set of evaluation augmentations so that we can do consistent evaluation across runs.* There are options to train a set number of epochs or with a fixed query budget (evaluation is not counted toward query budget). Use the latter to do apples-to-apples comparisons across different hyperparameter settings.* The log directory you choose for the trainer object will store TensorBoard summaries, a YAML file containing all the training details, and an exported numpy array of the perturbation at the end of every epoch.* If you pass an MLflow URI and experiment name to the trainer, it will auto-log all hyperparameters and record the eval transform robustness during training. You can also pass it a dictionary of extra hyperparameters (for example, details of how you initialized the perturbation) so that you can capture and visualize them in MLflow.* Kornia has a bunch of options we can use to make the transform robustness tests more realistic. The main one I've added is perspective warping, to simulate LPR cameras looking at the plate from an angle. It's a relatively easy lift to add extra augmentation options from the Kornia API.* We can also customize augmentations to simulate other sources of noise. I've added two augmentation parameters for compoosition noise, simulating errors in application of the patch by randomly shifting and rotating it a bit relative to the target image.* Implementation details* User needs to wrap the black-box model in a function that:  * inputs a pytorch tensor in (C,H,W) format  * returns 1 if the model got the right answer, 0 if the model got the wrong answer, and -1 if it threw an error. so our goal is to get as many 0's as possible.* TR calculation is parallelized with dask. So outside of this code you can globally configure the parallelization using dask.config.set() to use threads, processes, or distribute across a cluster.* Trainer can input a pre-initialized perturbation as a pytorch tensor in (C',H',W') format. If you want a greyscale patch use C=1 instead of 3. The perturbation will be resized during training to the size of the image we're attacking, so you can train on a lower-resolution patch. You could also use this to continue training from an earlier patch.* GRAPHITE code has options for fixed learning rate and adaptive learning rate using backtracing line search. The BLS code looks a little different from other implementations of the algorithm I've seen (and, of course, has no comments). Until I figure out exactly what they're doing, I have an in-between stopgap that just samples a few learning rates on an approximately logarithmic scale. * The GRAPHITE code appears to sample one set of training augmentations and reuse them- I'm worried this could make it easy to overfit to augmentation sampling noise. My code, by default, stores hyperparameters for the distribution augmentations are drawn from, and redraws the actual augmentations each step (since that takes basically no time). So the transformations used to estimate the gradient are decorrelated from the ones used to pick a step size. You can also give it a list of pre-chosen augmentations and at each step it will sample from them (with replacement).* GRAPHITE introduces a complicated set of heuristics for learning to optimize the patch mask. You need that if you're learning a mask as part of the optimization problem, but in our case we already know the final shape. I replaced this with a cheaper algorithm that interpolates between initial and final patches, using a binary search to get as close as possible to a target TR (default 0.5). Once it's close to the final mask it swaps to that one and skips this step.* The ALPR file contains code for interacting with OpenALPR two different ways, both leveraging docker.   * The OOP wrapper writes the modified image to disk and uses subprocess to call OpenALPR. This comes with significant overhead for spinning up and shutting down a docker container each time (>half a second), which, along with I/O winds up being most of the time spent. Theoretically it's possible to use subprocess or docker-py to maintain a persistent container with the entry point mapped to bash, and send it images without the spin up/down time but I haven't gotten it to work.  * The functional wrapper interacts with a docker container that runs OpenALPR as a REST API and sends the images using BytesIO so it doesn't have to touch the disk. Significantly faster.Preliminary resultsSome things I think I've learned so far. Anything here should be tested on a larger dataset of license plates to be sure.* Assume your perturbation isn't robust against any noise you haven't tested against. GRAPHITE with parameters close to the paper sees a big drop in performance when you evaluate against composition noise. In this case, training with that noise, or training lower-resolution perturbations, seems to help.* Use lower-resolution perturbations. The GRAPHITE paper references getting better results with the patch dimensions downsampled by 2- I've gotten improvements going significantly further (e.g. downsampling by 8). A couple hypothesis for why:  * Acts as a low-pass filter that prevents it from learning more brittle high-frequency patterns  * One term in the variance of the RGF gradient estimator scales cubicly with the dimension of the space we're optimizing over. Downsampling by 8 would reduce the size of this term by five orders of magnitude.* Before training, evaluate transform robustness using an untrained patch. OpenALPR is brittle and just adding a new set of lines around the plate can mess with it in some instances. If a gray frame fools ALPR 80% of the time, then four out of five calculations you run during training aren't giving you any useful signal for improving your patch. You can constrain the training augmentations to useful cases but keep the eval augmentations mroe broad to be safe.Known Issues* When the trainer object goes out of scope it should notify the MLflow server to stop the current run. If you run trainers in a loop, though, sometimes the server gets confused. Deleting the old trainer and adding a 1-second pause before the end of the loop seems to work fine.Future Directions* Add an API to make it easy to add more custom TensorBoard summaries. Start keeping track of how often OpenALPR returns nothing at all, measure Levenshtein distance from the correct answer, etc.* What do we need to add to optimize hyperparameters over a dataset of targets for a given domain (e.g. a bunch of license plates)?* We're currently doing approximate gradient descent directly on the space of possible patches. A possible interpretation of the downsampling experiments is that the space of useful patches may exist on a predictable manifold of much lower dimension than that ambient space. Given the steep tradeoff between sampling dimension and variance of the multi-point gradiente estimator, anything we can do to reduce that dimension (e.g. picking a better distribution to sample from) will significantly increae our efficiency.  * The theoretical results for multi-point gradient estimation generally require the samples to be IID and symmetric (e.g. rotationally invariant in the input space). * Integrate Bayesian optimization for picking hyperparameters